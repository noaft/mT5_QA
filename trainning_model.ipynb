{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":71903,"status":"ok","timestamp":1728732305456,"user":{"displayName":"Minh Tran","userId":"05290417497755930075"},"user_tz":-420},"id":"3seyXLa5z23g","outputId":"f2fc9b64-8e3f-4a89-faa1-de07cdd53a28"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19575,"status":"ok","timestamp":1728732325022,"user":{"displayName":"Minh Tran","userId":"05290417497755930075"},"user_tz":-420},"id":"m0FOJY8YiAoV","outputId":"40464148-81ab-467c-9030-af1f490ee108"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'transformers'...\n","remote: Enumerating objects: 192433, done.\u001b[K\n","remote: Counting objects: 100% (21419/21419), done.\u001b[K\n","remote: Compressing objects: 100% (1422/1422), done.\u001b[K\n","remote: Total 192433 (delta 21030), reused 20004 (delta 19997), pack-reused 171014 (from 1)\u001b[K\n","Receiving objects: 100% (192433/192433), 210.45 MiB | 16.60 MiB/s, done.\n","Resolving deltas: 100% (139354/139354), done.\n"]}],"source":["!git clone https://github.com/noaft/transformers.git\n","!cd transformers/examples/pytorch/question-answering/"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4560,"status":"ok","timestamp":1728732329576,"user":{"displayName":"Minh Tran","userId":"05290417497755930075"},"user_tz":-420},"id":"4U5GGpJK2khb","outputId":"8140740a-5f9d-4913-9cbe-6b36c4e76383"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: accelerate>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (0.34.2)\n","Collecting datasets>=1.8.0 (from -r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2))\n","  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (2.4.1+cu121)\n","Collecting evaluate (from -r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 4))\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (24.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (6.0.2)\n","Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (0.24.7)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (0.4.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (3.16.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (16.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2))\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (4.66.5)\n","Collecting xxhash (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2))\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2))\n","  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (3.10.9)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (3.1.4)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (1.13.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (2.1.5)\n","INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2024.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (1.16.0)\n","Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets, evaluate\n","Successfully installed datasets-3.0.1 dill-0.3.8 evaluate-0.4.3 multiprocess-0.70.16 xxhash-3.5.0\n"]}],"source":["!pip install -r /content/transformers/examples/pytorch/question-answering/requirements.txt"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21259,"status":"ok","timestamp":1728732350809,"user":{"displayName":"Minh Tran","userId":"05290417497755930075"},"user_tz":-420},"id":"V_V6a0IZ3XXT","outputId":"79a7c50d-9b75-4a9c-802f-473dca04ff7b"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/transformers\n","Processing /content/transformers\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (2.32.3)\n","Collecting tokenizers<0.21,>=0.20 (from transformers==4.46.0.dev0)\n","  Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (0.4.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.46.0.dev0) (4.66.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0.dev0) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.46.0.dev0) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0.dev0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0.dev0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0.dev0) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.46.0.dev0) (2024.8.30)\n","Downloading tokenizers-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: transformers\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.46.0.dev0-py3-none-any.whl size=9912146 sha256=572d20e1e438d07b8343d306fa248b48c268549be190eacacb35c53b6dc709d3\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ql7wifti/wheels/7c/35/80/e946b22a081210c6642e607ed65b2a5b9a4d9259695ee2caf5\n","Successfully built transformers\n","Installing collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.19.1\n","    Uninstalling tokenizers-0.19.1:\n","      Successfully uninstalled tokenizers-0.19.1\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.44.2\n","    Uninstalling transformers-4.44.2:\n","      Successfully uninstalled transformers-4.44.2\n","Successfully installed tokenizers-0.20.1 transformers-4.46.0.dev0\n"]}],"source":["%cd transformers\n","!pip install ."]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oLMgAR6T1mCG","executionInfo":{"status":"ok","timestamp":1728743426723,"user_tz":-420,"elapsed":51846,"user":{"displayName":"Minh Tran","userId":"05290417497755930075"}},"outputId":"300464be-3666-43ae-a448-51a0746c272f"},"outputs":[{"output_type":"stream","name":"stdout","text":["2024-10-12 14:29:41.361539: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-10-12 14:29:41.383502: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-10-12 14:29:41.390204: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-10-12 14:29:42.578093: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","10/12/2024 14:29:44 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n","10/12/2024 14:29:44 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=1,\n","accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","batch_eval_metrics=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_persistent_workers=False,\n","dataloader_pin_memory=True,\n","dataloader_prefetch_factor=None,\n","ddp_backend=None,\n","ddp_broadcast_buffers=None,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","dispatch_batches=None,\n","do_eval=True,\n","do_predict=False,\n","do_train=False,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_do_concat_batches=True,\n","eval_on_start=False,\n","eval_steps=None,\n","eval_strategy=no,\n","eval_use_gather_object=False,\n","evaluation_strategy=None,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_config=None,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","gradient_checkpointing_kwargs=None,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_always_push=False,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_for_metrics=[],\n","include_inputs_for_metrics=False,\n","include_num_input_tokens_seen=False,\n","include_tokens_per_second=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=3e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=0,\n","log_level=passive,\n","log_level_replica=warning,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/Tranning_MT5/runs/Oct12_14-29-44_e18af0c64dde,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_kwargs={},\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","neftune_noise_alpha=None,\n","no_cuda=False,\n","num_train_epochs=20.0,\n","optim=adamw_torch,\n","optim_args=None,\n","optim_target_modules=None,\n","output_dir=/content/drive/MyDrive/Tranning_MT5/,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=12,\n","predict_with_generate=False,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","restore_callback_states_from_checkpoint=False,\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/Tranning_MT5/,\n","save_on_each_node=False,\n","save_only_model=False,\n","save_safetensors=True,\n","save_steps=1000,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","split_batches=None,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torch_empty_cache_steps=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_cpu=False,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_liger_kernel=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n",")\n","Using custom data configuration default-33b83a1f3a2af362\n","10/12/2024 14:29:45 - INFO - datasets.builder - Using custom data configuration default-33b83a1f3a2af362\n","Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","10/12/2024 14:29:45 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n","Overwrite dataset info from restored data version if exists.\n","10/12/2024 14:29:45 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-33b83a1f3a2af362/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","10/12/2024 14:29:45 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-33b83a1f3a2af362/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","Found cached dataset json (/root/.cache/huggingface/datasets/json/default-33b83a1f3a2af362/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","10/12/2024 14:29:45 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-33b83a1f3a2af362/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n","Loading Dataset info from /root/.cache/huggingface/datasets/json/default-33b83a1f3a2af362/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","10/12/2024 14:29:45 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-33b83a1f3a2af362/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n","[INFO|configuration_utils.py:673] 2024-10-12 14:29:45,622 >> loading configuration file /content/drive/MyDrive/Tranning_MT5/checkpoint-12000/config.json\n","[INFO|configuration_utils.py:742] 2024-10-12 14:29:45,626 >> Model config MT5Config {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Tranning_MT5/checkpoint-12000\",\n","  \"architectures\": [\n","    \"MT5ForConditionalGeneration\"\n","  ],\n","  \"classifier_dropout\": 0.0,\n","  \"d_ff\": 2048,\n","  \"d_kv\": 64,\n","  \"d_model\": 768,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"gelu_new\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"gated-gelu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"mt5\",\n","  \"num_decoder_layers\": 12,\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"tie_word_embeddings\": false,\n","  \"tokenizer_class\": \"T5Tokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.46.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250112\n","}\n","\n","[INFO|configuration_utils.py:675] 2024-10-12 14:29:45,896 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-base/snapshots/2eb15465c5dd7f72a8f7984306ad05ebc3dd1e1f/config.json\n","[INFO|configuration_utils.py:742] 2024-10-12 14:29:45,896 >> Model config MT5Config {\n","  \"_name_or_path\": \"google/mt5-base\",\n","  \"architectures\": [\n","    \"MT5ForConditionalGeneration\"\n","  ],\n","  \"classifier_dropout\": 0.0,\n","  \"d_ff\": 2048,\n","  \"d_kv\": 64,\n","  \"d_model\": 768,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"gelu_new\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"gated-gelu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"mt5\",\n","  \"num_decoder_layers\": 12,\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"tie_word_embeddings\": false,\n","  \"tokenizer_class\": \"T5Tokenizer\",\n","  \"transformers_version\": \"4.46.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250112\n","}\n","\n","[INFO|tokenization_utils_base.py:2206] 2024-10-12 14:29:45,897 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google--mt5-base/snapshots/2eb15465c5dd7f72a8f7984306ad05ebc3dd1e1f/spiece.model\n","[INFO|tokenization_utils_base.py:2206] 2024-10-12 14:29:45,897 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:2206] 2024-10-12 14:29:45,897 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2206] 2024-10-12 14:29:45,897 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--mt5-base/snapshots/2eb15465c5dd7f72a8f7984306ad05ebc3dd1e1f/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:2206] 2024-10-12 14:29:45,897 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-base/snapshots/2eb15465c5dd7f72a8f7984306ad05ebc3dd1e1f/tokenizer_config.json\n","[INFO|configuration_utils.py:675] 2024-10-12 14:29:45,898 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-base/snapshots/2eb15465c5dd7f72a8f7984306ad05ebc3dd1e1f/config.json\n","[INFO|configuration_utils.py:742] 2024-10-12 14:29:45,898 >> Model config MT5Config {\n","  \"_name_or_path\": \"google/mt5-base\",\n","  \"architectures\": [\n","    \"MT5ForConditionalGeneration\"\n","  ],\n","  \"classifier_dropout\": 0.0,\n","  \"d_ff\": 2048,\n","  \"d_kv\": 64,\n","  \"d_model\": 768,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"gelu_new\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"gated-gelu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"mt5\",\n","  \"num_decoder_layers\": 12,\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"tie_word_embeddings\": false,\n","  \"tokenizer_class\": \"T5Tokenizer\",\n","  \"transformers_version\": \"4.46.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250112\n","}\n","\n","[WARNING|logging.py:328] 2024-10-12 14:29:46,362 >> You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","[INFO|configuration_utils.py:675] 2024-10-12 14:29:47,700 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-base/snapshots/2eb15465c5dd7f72a8f7984306ad05ebc3dd1e1f/config.json\n","[INFO|configuration_utils.py:742] 2024-10-12 14:29:47,701 >> Model config MT5Config {\n","  \"_name_or_path\": \"google/mt5-base\",\n","  \"architectures\": [\n","    \"MT5ForConditionalGeneration\"\n","  ],\n","  \"classifier_dropout\": 0.0,\n","  \"d_ff\": 2048,\n","  \"d_kv\": 64,\n","  \"d_model\": 768,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"gelu_new\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"gated-gelu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"mt5\",\n","  \"num_decoder_layers\": 12,\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"tie_word_embeddings\": false,\n","  \"tokenizer_class\": \"T5Tokenizer\",\n","  \"transformers_version\": \"4.46.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250112\n","}\n","\n","/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","[INFO|modeling_utils.py:3755] 2024-10-12 14:29:48,486 >> loading weights file /content/drive/MyDrive/Tranning_MT5/checkpoint-12000/model.safetensors\n","[INFO|configuration_utils.py:1099] 2024-10-12 14:29:48,779 >> Generate config GenerationConfig {\n","  \"decoder_start_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"pad_token_id\": 0\n","}\n","\n","[INFO|modeling_utils.py:4620] 2024-10-12 14:29:52,076 >> All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:4628] 2024-10-12 14:29:52,077 >> All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/Tranning_MT5/checkpoint-12000.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n","[INFO|configuration_utils.py:1052] 2024-10-12 14:29:52,178 >> loading configuration file /content/drive/MyDrive/Tranning_MT5/checkpoint-12000/generation_config.json\n","[INFO|configuration_utils.py:1099] 2024-10-12 14:29:52,178 >> Generate config GenerationConfig {\n","  \"decoder_start_token_id\": 0,\n","  \"eos_token_id\": 1,\n","  \"pad_token_id\": 0\n","}\n","\n","Running tokenizer on validation dataset:   0% 0/2285 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-33b83a1f3a2af362/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-bd4dc1fde90814ef.arrow\n","10/12/2024 14:29:52 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-33b83a1f3a2af362/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-bd4dc1fde90814ef.arrow\n","Running tokenizer on validation dataset: 100% 2285/2285 [00:02<00:00, 1085.21 examples/s]\n","10/12/2024 14:29:57 - INFO - __main__ - *** Evaluate ***\n","[INFO|trainer.py:856] 2024-10-12 14:29:57,664 >> The following columns in the evaluation set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n","[INFO|trainer.py:4054] 2024-10-12 14:29:58,065 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4056] 2024-10-12 14:29:58,065 >>   Num examples = 2514\n","[INFO|trainer.py:4059] 2024-10-12 14:29:58,065 >>   Batch size = 8\n"," 23% 74/315 [00:25<02:35,  1.55it/s]Traceback (most recent call last):\n","  File \"/content/transformers/./examples/pytorch/question-answering/run_seq2seq_qa.py\", line 759, in <module>\n","    main()\n","  File \"/content/transformers/./examples/pytorch/question-answering/run_seq2seq_qa.py\", line 718, in main\n","    metrics = trainer.evaluate(max_length=max_length, num_beams=num_beams, metric_key_prefix=\"eval\")\n","  File \"/content/transformers/examples/pytorch/question-answering/trainer_seq2seq_qa.py\", line 69, in evaluate\n","    output = eval_loop(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 4122, in evaluation_loop\n","    all_preds.add(logits)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\", line 322, in add\n","    self.tensors = nested_concat(self.tensors, tensors, padding_index=self.padding_index)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\", line 134, in nested_concat\n","    return type(tensors)(nested_concat(t, n, padding_index=padding_index) for t, n in zip(tensors, new_tensors))\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\", line 134, in <genexpr>\n","    return type(tensors)(nested_concat(t, n, padding_index=padding_index) for t, n in zip(tensors, new_tensors))\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\", line 136, in nested_concat\n","    return torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_index)\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\", line 94, in torch_pad_and_concatenate\n","    return torch.cat((tensor1, tensor2), dim=0)\n","torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 16.77 GiB. GPU 0 has a total capacity of 39.56 GiB of which 16.46 GiB is free. Process 825813 has 23.10 GiB memory in use. Of the allocated memory 19.95 GiB is allocated by PyTorch, and 2.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"," 23% 74/315 [00:26<01:26,  2.78it/s]\n"]}],"source":["\n","!python ./examples/pytorch/question-answering/run_seq2seq_qa.py \\\n","  --model_name_or_path /content/drive/MyDrive/Tranning_MT5/checkpoint-12000 \\\n","  --tokenizer_name google/mt5-base \\\n","  --train_file /content/drive/MyDrive/ViQuAD1.0/train_ViQuAD__.json \\\n","  --validation_file /content/drive/MyDrive/ViQuAD1.0/dev_ViQuAD__.json \\\n","  --do_eval \\\n","  --per_device_train_batch_size 12 \\\n","  --learning_rate 3e-5 \\\n","  --num_train_epochs 20 \\\n","  --save_steps 1000 \\\n","  --overwrite_output_dir \\\n","  --max_seq_length 584 \\\n","  --doc_stride 128 \\\n","  --output_dir /content/drive/MyDrive/Tranning_MT5/\n"]},{"cell_type":"code","source":["import torch\n","from transformers import MT5ForConditionalGeneration, T5Tokenizer\n","\n","def answer_question(question, context, model, tokenizer):\n","    # Chuẩn bị input cho mô hình mT5\n","    input_text = f\"question: {question} context: {context}\"\n","    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n","\n","    # Generate output từ mô hình mT5\n","    with torch.no_grad():\n","        output_ids = model.generate(inputs['input_ids'], max_length=150, num_beams=4, early_stopping=True)\n","\n","    # Decode output thành câu trả lời\n","    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","    return answer\n","\n","# Load mô hình mT5 pre-trained và tokenizer\n","model_name = \"/content/drive/MyDrive/Tranning_MT5/checkpoint-12000\"  # Hoặc đường dẫn tới mô hình đã huấn luyện\n","model = MT5ForConditionalGeneration.from_pretrained(model_name)\n","tokenizer = T5Tokenizer.from_pretrained('google/mt5-small')\n","\n","# Ví dụ câu hỏi và ngữ cảnh\n","question = \"Toàn bao nhiêu tuổi?\"\n","context = \"Tôi tên Toàn, sinh sống ở Gia Lai, sinh năm 2003 Toàn năm nay học lớp 12 và 18 tuổi\"\n","\n","# Tìm câu trả lời từ mô hình\n","answer = answer_question(question, context, model, tokenizer)\n","\n","print(f\"Question: {question}\")\n","print(f\"Answer: {answer}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IJ04OyqyU6Gq","executionInfo":{"status":"ok","timestamp":1728740006875,"user_tz":-420,"elapsed":7270,"user":{"displayName":"Minh Tran","userId":"05290417497755930075"}},"outputId":"d9f8a0b4-2554-4cb8-d18b-382d4c9f5783"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Question: Toàn bao nhiêu tuổi?\n","Answer: 18 tuổi\n"]}]}],"metadata":{"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm","authorship_tag":"ABX9TyMwV/Hgo8U78v+//i/6tg+0"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}